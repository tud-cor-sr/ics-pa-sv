{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac408b5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a96d02d6346fa7399cf6da0111ce0937",
     "grade": false,
     "grade_id": "cell-b00828259c8e42e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# RO47019: Intelligent Control Systems Practical Assignment\n",
    "* Period: 2022-2023, Q3\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/500969\n",
    "* Instructor: Cosimo Della Santina (C.DellaSantina@tudelft.nl)\n",
    "* Teaching assistant: Ruben Martin Rodriguez (R.MartinRodriguez@student.tudelft.nl)\n",
    "* (c) TU Delft, 2023\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Remove `raise NotImplementedError()` afterwards. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names, student numbers, netID, and emails below.\n",
    "STUDENT_1_NAME = \"\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"\"\n",
    "STUDENT_1_NETID = \"\"\n",
    "STUDENT_1_EMAIL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba32571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "042927213b84aa368aa3ea72caa4cb60",
     "grade": true,
     "grade_id": "cell-9f148ec62e0de49c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert STUDENT_1_NAME != \"\"\n",
    "assert STUDENT_1_STUDENT_NUMBER != \"\"\n",
    "assert STUDENT_1_NETID != \"\"\n",
    "assert STUDENT_1_EMAIL != \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af317a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76ef40fcc3f08a0484661497162a1a9",
     "grade": false,
     "grade_id": "cell-4ea391677951116c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled Q&A hours to ask the TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "860bf3fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f04550e899e89ccc4ebeeadc13039165",
     "grade": false,
     "grade_id": "cell-60a0ce6bc7eb39ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 1 - Vision-based angle prediction (42.5p)\n",
    "\n",
    "**Authors:** Tomás Coleman (T.Coleman@tudelft.nl), Chuhan Zhang (C.Zhang-8@tudelft.nl)\n",
    "\n",
    "\n",
    "The following cells import all the necessary packages and external functions to properly run the code. Additionally, different dataset classes are created from the information gathered in the notebook 1a. Finally, different Pytorch data loaders will be created for each network architecture which will be introduced throughout the notebook where corresponds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dae7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloads the python files outside of this notebook automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import all Python modules\n",
    "from distutils.util import strtobool\n",
    "from jax.config import config as jax_config\n",
    "\n",
    "jax_config.update(\"jax_platform_name\", \"cpu\")  # set default device to 'cpu'\n",
    "jax_config.update(\"jax_enable_x64\", True)  # double precision\n",
    "from jax import random\n",
    "from jax import numpy as jnp\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# define boolean to check if the notebook is run for the purposes of autograding\n",
    "AUTOGRADING = strtobool(os.environ.get(\"AUTOGRADING\", \"false\"))\n",
    "\"\"\n",
    "# create directory for datasets\n",
    "datasets_dir = Path(\"datasets\")\n",
    "datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create directory for plots\n",
    "outputs_dir = Path(\"outputs\")\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create directory for state dictionaries of neural network\n",
    "statedicts_dir = Path(\"statedicts\")\n",
    "statedicts_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThetaDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.x = torch.tensor(dataset[\"th_pix_curr\"], dtype=torch.float32) / 255.0\n",
    "        self.y = torch.tensor(dataset[\"th_curr_ss\"], dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        num_samples = self.x.shape[0]\n",
    "\n",
    "        return num_samples\n",
    "\n",
    "\n",
    "class TrigDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.x = torch.tensor(dataset[\"th_pix_curr\"], dtype=torch.float32) / 255.0\n",
    "        y_cos = torch.cos(torch.tensor(dataset[\"th_curr_ss\"], dtype=torch.float32))\n",
    "        y_sin = torch.sin(torch.tensor(dataset[\"th_curr_ss\"], dtype=torch.float32))\n",
    "        self.y = torch.cat((y_sin, y_cos), dim=-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        num_samples = self.x.shape[0]\n",
    "\n",
    "        return num_samples\n",
    "\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.x = (\n",
    "            torch.tensor(\n",
    "                onp.transpose(dataset[\"th_pix_curr\"], (0, 3, 1, 2)), dtype=torch.float32\n",
    "            )\n",
    "            / 255.0\n",
    "        )\n",
    "        y_cos = torch.cos(torch.tensor(dataset[\"th_curr_ss\"], dtype=torch.float32))\n",
    "        y_sin = torch.sin(torch.tensor(dataset[\"th_curr_ss\"], dtype=torch.float32))\n",
    "        self.y = torch.cat((y_sin, y_cos), dim=-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        num_samples = self.x.shape[0]\n",
    "\n",
    "        return num_samples\n",
    "\n",
    "\n",
    "class SNNDataset(Dataset):\n",
    "    def __init__(self, path, num_itr, num_data):\n",
    "        self.path = path\n",
    "        self.num_itr = num_itr\n",
    "        self.num_data = num_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.num_itr * self.num_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < int(self.num_data * self.num_itr):\n",
    "            spike_path = \"spike\" + str(index) + \".pt\"\n",
    "            label_path = \"target\" + str(index) + \".pt\"\n",
    "            spike_out = torch.load(os.path.join(self.path, spike_path))\n",
    "            label_out = torch.load(os.path.join(self.path, label_path))\n",
    "            return spike_out, label_out\n",
    "        else:\n",
    "            raise IndexError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_dataset_dataloader(\n",
    "    _filepath, _rng: random.KeyArray, _val_ratio=0.3, batch_size=64, model_type=\"theta\"\n",
    "):\n",
    "    assert 0.0 <= _val_ratio <= 1.0, \"Validation ratio needs to be in interval [0, 1].\"\n",
    "\n",
    "    _dataset = jnp.load(_filepath)\n",
    "    num_samples = _dataset[\"th_curr_ss\"].shape[0]\n",
    "\n",
    "    indices = jnp.arange(num_samples)\n",
    "    shuffled_indices = random.permutation(_rng, indices)\n",
    "    num_train_samples = int((1 - _val_ratio) * num_samples)\n",
    "    split_config = jnp.array(\n",
    "        [\n",
    "            num_train_samples,\n",
    "        ]\n",
    "    )\n",
    "    train_indices, val_indices = jnp.split(shuffled_indices, split_config)\n",
    "\n",
    "    _train_ds, _val_ds = {}, {}\n",
    "    for key, val in _dataset.items():\n",
    "        _train_ds[key] = val[train_indices]\n",
    "        _val_ds[key] = val[val_indices]\n",
    "\n",
    "    if model_type == \"theta\":\n",
    "        train_data = ThetaDataset(_train_ds)\n",
    "        val_data = ThetaDataset(_val_ds)\n",
    "    elif model_type == \"trig\":\n",
    "        train_data = TrigDataset(_dataset)\n",
    "        val_data = TrigDataset(_val_ds)\n",
    "    elif model_type == \"cnn\":\n",
    "        train_data = CNNDataset(_dataset)\n",
    "        val_data = CNNDataset(_val_ds)\n",
    "\n",
    "    _train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    _val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_data, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    return _train_dataloader, _val_dataloader\n",
    "\n",
    "\n",
    "def load_test_dataset_dataloader(\n",
    "    _filepath, _rng: random.KeyArray, batch_size=64, model_type=\"theta\"\n",
    "):\n",
    "    _dataset = jnp.load(_filepath)\n",
    "    num_samples = _dataset[\"th_curr_ss\"].shape[0]\n",
    "\n",
    "    if model_type == \"theta\":\n",
    "        print(\"Theta Dataset\")\n",
    "        _data = ThetaDataset(_dataset)\n",
    "\n",
    "    elif model_type == \"trig\":\n",
    "        print(\"Trig Dataset\")\n",
    "        _data = TrigDataset(_dataset)\n",
    "    elif model_type == \"cnn\":\n",
    "        print(\"CNN dataset\")\n",
    "        _data = CNNDataset(_dataset)\n",
    "\n",
    "    _dataloader = torch.utils.data.DataLoader(\n",
    "        _data, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    return _dataloader, _dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7158c49",
   "metadata": {},
   "source": [
    "One last function, evaluate_model, is also defined to evaluate the outcomes of the different models in their respective datasets and display a heatmap of the prediction errors. You do not have to worry about its workings nor interface since you are not required to use it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    _model, test_dataset, model=\"theta\", file=\"nothing.pdf\", num_bins: int = 10\n",
    "):\n",
    "    filepath = str(outputs_dir / file)\n",
    "    th = test_dataset[\"th_curr_ss\"]\n",
    "\n",
    "    # The pixel values need to be divided by 255.0 to match the input of the dataset classes\n",
    "    if model == \"cnn\":\n",
    "        x = onp.transpose(\n",
    "            onp.array(test_dataset[\"th_pix_curr\"] / 255.0, dtype=onp.float32),\n",
    "            (0, 3, 1, 2),\n",
    "        )\n",
    "        x = torch.tensor(x)\n",
    "        x_len = x.size()[0]\n",
    "        batch_size = 100\n",
    "        batch_num = int(int(x_len) / int(batch_size))\n",
    "        y_hat = torch.zeros((0, 4))\n",
    "        for i in range(batch_num):\n",
    "            x_batch = x[int(i * batch_size) : int((i + 1) * batch_size)]\n",
    "            y_batch = _model(x_batch).detach()\n",
    "            y_hat = torch.cat((y_hat, y_batch), dim=0)\n",
    "        y_hat = y_hat.numpy()\n",
    "    else:\n",
    "        y_hat = (\n",
    "            _model(\n",
    "                torch.tensor(\n",
    "                    onp.array(test_dataset[\"th_pix_curr\"] / 255.0, dtype=onp.float32)\n",
    "                )\n",
    "            )\n",
    "            .detach()\n",
    "            .numpy()\n",
    "        )  # need to convert back to np array\n",
    "\n",
    "    if model == \"theta\":\n",
    "        y = th\n",
    "    else:\n",
    "        y_cos = onp.cos(th)\n",
    "        y_sin = onp.sin(th)\n",
    "\n",
    "        y = onp.concatenate((y_sin, y_cos), axis=-1)\n",
    "\n",
    "        y = trig_to_theta_numpy(y)\n",
    "        y_hat = trig_to_theta_numpy(y_hat)\n",
    "\n",
    "    error = y_hat - y\n",
    "    error = onp.abs(error)\n",
    "\n",
    "    extent = onp.array(\n",
    "        [\n",
    "            [-onp.pi, onp.pi],\n",
    "            [-onp.pi, onp.pi],\n",
    "        ]\n",
    "    )\n",
    "    heatmap, xedges, yedges = onp.histogram2d(\n",
    "        x=th[:, 0], y=th[:, 1], bins=num_bins, range=extent\n",
    "    )\n",
    "\n",
    "    avg_bins = onp.zeros((num_bins, num_bins))\n",
    "    for i in range(avg_bins.shape[0]):\n",
    "        if i == 0:\n",
    "            # also include the left edge for the first bin\n",
    "            xcond = (xedges[i] <= th[:, 0]) & (th[:, 0] <= xedges[i + 1])\n",
    "        else:\n",
    "            xcond = (xedges[i] < th[:, 0]) & (th[:, 0] <= xedges[i + 1])\n",
    "\n",
    "        for j in range(avg_bins.shape[1]):\n",
    "            if j == 0:\n",
    "                # also include the left edge for the first bin\n",
    "                ycond = (yedges[j] <= th[:, 1]) & (th[:, 1] <= yedges[j + 1])\n",
    "            else:\n",
    "                ycond = (yedges[j] < th[:, 1]) & (th[:, 1] <= yedges[j + 1])\n",
    "\n",
    "            # combine condition for th1 and th2\n",
    "            bin_cond = xcond & ycond\n",
    "\n",
    "            # save the average error for each bin\n",
    "            avg_bins[i, j] = onp.mean(error[bin_cond, :])\n",
    "\n",
    "    print(\"Mean prediction error across bins:\", onp.mean(avg_bins))\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    plt.imshow(avg_bins, extent=extent.flatten())\n",
    "    plt.title(\"Heatmap of prediction error\")\n",
    "    plt.xlabel(\"link 1 angles (rad)\")\n",
    "    plt.ylabel(\"link 2 angles (rad)\")\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def trig_to_theta(trig_data):\n",
    "    trig_len = trig_data.shape[0]\n",
    "    theta = torch.zeros(trig_len, 2)\n",
    "    for i in range(trig_len):\n",
    "        theta[i, 0] = torch.atan2(trig_data[i, 0], trig_data[i, 2])\n",
    "        theta[i, 1] = torch.atan2(trig_data[i, 1], trig_data[i, 3])\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Create a conversion function that supports numpy arrays instead of torch tensors\n",
    "def trig_to_theta_numpy(trig_data):\n",
    "    trig_len = trig_data.shape[0]\n",
    "    theta = onp.zeros((trig_len, 2))\n",
    "    for i in range(trig_len):\n",
    "        theta[i, 0] = onp.arctan2(trig_data[i, 0], trig_data[i, 2])\n",
    "        theta[i, 1] = onp.arctan2(trig_data[i, 1], trig_data[i, 3])\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5099e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for jax\n",
    "rng = random.PRNGKey(seed=42)\n",
    "rng, init_rng = random.split(rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3321e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4713f43e52cf23b0b368340cb88e19d4",
     "grade": false,
     "grade_id": "cell-ed9777984ea2045a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.1: Learn to predict angles (7.5p)\n",
    "\n",
    "We are going to create models that try to predict the link angles $\\hat{\\theta}$ given an image of the robot, so $\\theta \\approx \\hat{\\theta} =M(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44751fbe",
   "metadata": {},
   "source": [
    "### 1.1 - Prepare data \n",
    "Run the functions below to load the data generated in `task_1a_generate_data.ipynb`, split the training into the 70/30 train/validation split, set the batch size to and put them in a dataloader. The data is first put in the class `ThetaDataset`. This class converts the `uint8` pixel values from a range of 0-255 to float values in the range `0-1`. The class also provides methods for the dataloader to retreive the pixel obervations `.x` and test labels `y`. The dataloader is set to Shuffle the data randomly before enumerating during the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18479886",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    train_loader_theta, val_loader_theta = load_training_dataset_dataloader(\n",
    "        str(datasets_dir / \"dataset_double_pendulum_train.npz\"),\n",
    "        _rng=init_rng,\n",
    "        model_type=\"theta\",\n",
    "    )\n",
    "    test_loader_theta, _ = load_test_dataset_dataloader(\n",
    "        str(datasets_dir / \"dataset_double_pendulum_test.npz\"),\n",
    "        _rng=init_rng,\n",
    "        model_type=\"theta\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085abb0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "894837df8e313f3c435880002725816d",
     "grade": false,
     "grade_id": "cell-d9964f8bdfdb040b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 - Create the model (1.5p)\n",
    "Here, create a PyTorch model class called NeuralNetworkTheta()\n",
    "- Start by flattening the input with `torch.nn.Flatten()`.\n",
    "- Then, add a fully connected hidden layer `torch.nn.Linear(...)` of 128 units and ReLU activation.\n",
    "- Finally, add a final fully connected linear layer `torch.nn.Linear(...)`without activation. The number of units must match the dimension of our target data, which is 2, as we try to predict the angles $\\theta_1$ and $\\theta_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de9844",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c3764b0ee4465de155238df67983389",
     "grade": true,
     "grade_id": "cell-a14f93a12ba0f1a6",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NN Model with Theta as the output\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" TASK 1.1: CREATE MODEL HERE \"\"\"\n",
    "\n",
    "\n",
    "class NeuralNetworkTheta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\"\"\"TASK1.1: END\"\"\"\n",
    "model_theta = NeuralNetworkTheta()\n",
    "\n",
    "total_params_theta = sum(p.numel() for p in model_theta.parameters())\n",
    "print(\"total number of model parameters: \", total_params_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422298c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f058deeea9e14bada21355a98a90b119",
     "grade": false,
     "grade_id": "cell-c87a209ab1dddcd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 - Train the model (2p) \n",
    "\n",
    "Now that the model is defined, we can train it using the training dataset. We have a train-validation split of 0.3 and a batch size of 64 that is shuffled every epoch. Train the model for 100 epochs with the training data `train_loader_theta`. Tune the learning rate in the optimizer to give the best performance on the validation and test set over the 100 epochs. Do this 10 times and record the prediction error of the final model of each of the 10 runs on the test dataset `test_loader_theta` using the function `evaluate_model_test_data_theta`. For each training loop, it would be useful to test the model performance on the valedation set to see if it is overfitting on the training data or whether it can generalize well. **hint:** \n",
    "- reduce the number of epochs and lower the number of runs to `1` while getting your model working and setting the optimal learning rate. \n",
    "- The try variying the the learning rate in the range of `1e-1` to `1e-5`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef897c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "optimizer = torch.optim.SGD(model_theta.parameters(), lr=1e-3)\n",
    "# We choose the mean square loss as it is a regression problem\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "error_fn = torch.nn.L1Loss()\n",
    "num_epochs = 100\n",
    "\n",
    "# path where we will save the trained model\n",
    "model_theta_path = statedicts_dir / \"task_1-1_model_theta.pt\"\n",
    "\n",
    "\n",
    "def evaluate_model_test_data_theta(_model, _test_loader):\n",
    "    running_loss_test = 0.0\n",
    "    running_error_test = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data_test in enumerate(_test_loader):\n",
    "            count += 1\n",
    "            inputs_test, labels_test = data_test\n",
    "            batch = labels_test.shape[0]\n",
    "            est_th = _model(inputs_test)\n",
    "            loss_test = loss_fn(est_th, labels_test)\n",
    "            error_test = error_fn(est_th, labels_test)\n",
    "            running_loss_test += loss_test.item()\n",
    "            running_error_test += error_test.item()\n",
    "    print(\n",
    "        f\"Loss on test data: {running_loss_test / count:.3f}, Prediction error of model on test data: {running_error_test / count:.3f}\"\n",
    "    )\n",
    "    return running_error_test / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9564fe6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db142cc8f7c612dcd25ccf3001742887",
     "grade": true,
     "grade_id": "cell-460ecb4435520c92",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1.1: TRAIN MODEL HERE\"\"\"\n",
    "\n",
    "    num_runs = 10  # Change to 1 until you get it to work once\n",
    "    pred_error_theta = onp.zeros((num_runs))\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        # this code reinitializes the parameters of the model on each loop\n",
    "        model_theta = NeuralNetworkTheta()\n",
    "        optimizer = torch.optim.SGD(model_theta.parameters(), lr=1e-3)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        print(f\"run {run+1} finished\")\n",
    "        pred_error_theta[run] = evaluate_model_test_data_theta(\n",
    "            model_theta, test_loader_theta\n",
    "        )\n",
    "    print(\"average prediction error: \", onp.mean(pred_error_theta))\n",
    "    \"\"\"TASK 1.1: END\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e792bf1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d57f81ccc7046804b8bf09fe1318e07",
     "grade": false,
     "grade_id": "cell-7d7a283dd7bfcbba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If working on CodeSpaces, run this cell before running the next model to free up memory\n",
    "if not AUTOGRADING:\n",
    "    # save the jitted model of the last run\n",
    "    model_theta_scripted = torch.jit.script(model_theta)\n",
    "    model_theta_scripted.save(str(model_theta_path))\n",
    "\n",
    "    %xdel train_loader_theta\n",
    "    %xdel val_loader_theta\n",
    "    %xdel test_loader_theta\n",
    "    %xdel model_theta\n",
    "    %xdel model_theta_scripted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719a21f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0a8a7fb9e7972e833966bf8c8954d03",
     "grade": false,
     "grade_id": "cell-4126af15aa526be5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    # load jitted model of the last run\n",
    "    model_theta_scripted = torch.jit.load(str(model_theta_path))\n",
    "\n",
    "    _, test_dataset_theta = load_test_dataset_dataloader(\n",
    "        str(datasets_dir / \"dataset_double_pendulum_test.npz\"),\n",
    "        _rng=init_rng,\n",
    "        model_type=\"theta\",\n",
    "    )\n",
    "\n",
    "    evaluate_model(\n",
    "        model_theta_scripted,\n",
    "        test_dataset_theta,\n",
    "        model=\"theta\",\n",
    "        file=\"task_1-1_model_theta_prediction_error.pdf\",\n",
    "    )\n",
    "\n",
    "    %xdel model_theta_scripted\n",
    "    %xdel test_dataset_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3ea01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa4c2980ebf516a55e0a62a8bfc8d893",
     "grade": false,
     "grade_id": "cell-f481bd08fe4f5192",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 - Analyse Model performance (4p)\n",
    "Analyze the prediction accuracy of the model on the test data from heat map of the loss outputted from the `evaluate_model` function. Where does this model have the lowest accuracy? What could be an explanation for the loss of accuracy in those regions? **2p** (Answer in the cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f117ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50f464919ed852a42d638a59bf871076",
     "grade": true,
     "grade_id": "cell-f903416dd8d405dc",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e0173",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c233b9e1058b7cff5574c7eb9b58dc0",
     "grade": false,
     "grade_id": "cell-c2e81017b9edef72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In general, a separate test dataset is used to evaluate a trained model. Why? **(2p)** (Answer in the cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd26f9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "130572e156ddb38d897477e74fb544bb",
     "grade": true,
     "grade_id": "cell-349b357d7c6f715f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cc4a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a88dea86fd1937b546e8f22b7b3bb51",
     "grade": false,
     "grade_id": "cell-9767cd0ad15daeab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.2: Indirectly predict the angle\n",
    "We are going to improve the accuracy by pre-processing the target data. Specifically, we will create a model $M_{trig}$ that learns to predict $\\sin(\\theta)$ and $\\cos(\\theta)$ for both links instead of directly predicting $\\theta$. Then, we can use the trigonometric relation to retrieve an estimate of $\\theta$ for both links.\n",
    "\n",
    "Note: In practice you would use the `atan2` implementation, as the regular arctangent only covers $[-\\frac{1}{2}\\pi, \\frac{1}{2}\\pi]$} $\\theta=\\arctan(\\frac{\\sin(\\theta)}{\\cos(\\theta)})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcb3af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a04243746186bf36a665ae03c6280d7f",
     "grade": false,
     "grade_id": "cell-fed5df7188d95429",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Prepare the data (0p)\n",
    "\n",
    "Run the functions below to load the data generated in `task_1a_generate_data.ipynb`, split the training into the 70/30 train/validation split, set the batch size to and put them in a dataloader. The data is first put in the class `TrigDataset`. This class again converts the `uint8` pixel values from a range of 0-255 to float values in the range `0-1` while also calculating the values of $\\mathrm{sin}(\\theta_1), \\mathrm{sin}(\\theta_2), \\mathrm{cos}(\\theta_1), \\mathrm{cos}(\\theta_2)$ to be used as labels instead of just $\\theta_1, \\theta_2$ as was done in the `ThetaDataset` class. The dataloader is again set to shuffle the data randomly before enumerating during the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a6eac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd12ec1bfaaf87dc40c3b627f9e49cad",
     "grade": false,
     "grade_id": "cell-7cae12728e42a5af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    train_loader_trig, val_loader_trig = load_training_dataset_dataloader(\n",
    "        datasets_dir / \"dataset_double_pendulum_train.npz\",\n",
    "        _rng=init_rng,\n",
    "        model_type=\"trig\",\n",
    "    )\n",
    "    test_loader_trig, _ = load_test_dataset_dataloader(\n",
    "        datasets_dir / \"dataset_double_pendulum_test.npz\",\n",
    "        _rng=init_rng,\n",
    "        model_type=\"trig\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c678bdc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cfc4df64e3fe110b4a91b531a3636b5",
     "grade": false,
     "grade_id": "cell-3fd3a8ee34be422f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 - Create the model (1.5p)\n",
    "\n",
    "Copy the model you created in Task 1.1. Change the number of hidden units of the final layer from 2 to 4. We do so, because we now want to predict two outputs ($\\sin(\\theta)$, $\\cos(\\theta)$) for both link angles, instead of only two outputs, $\\theta_1$ and $\\theta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7145bd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2972028186e367adfc65233eb66731d5",
     "grade": true,
     "grade_id": "cell-0e4c6601038f04b2",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" TASK 1.2: CREATE MODEL HERE \"\"\"\n",
    "\n",
    "\n",
    "class NeuralNetworkTrig(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\"\"\"TASK1.2: END\"\"\"\n",
    "\n",
    "model_trig = NeuralNetworkTrig()\n",
    "total_params_trig = sum(p.numel() for p in model_trig.parameters())\n",
    "print(\"Total number of neural network parameters: \", total_params_trig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b0d23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1978bc0990aa84c66211ea1c2b30e7d",
     "grade": false,
     "grade_id": "cell-8050878591ff79e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 - Train the model (2p)\n",
    "\n",
    "Now train a model with the same training parameters defined below as in task 1.1 but using the training data `train_loader_trig`. Tune the learning rate in the optimizer to give the best performance on the validation and test set over the 100 epochs. Do this 10 times and record the prediction error of the final model of each of the 10 runs on the test dataset `test_loader_trig` using the function `evaluate_model_test_data_trig`. **Hint:** \n",
    "- reduce the number of epochs and lower the number of runs to `1` while getting your model working. \n",
    "- The try in the range `1e-1` to `1e-5` for the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "optimizer = torch.optim.SGD(model_trig.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "error_fn = torch.nn.L1Loss()\n",
    "num_epochs = 100\n",
    "\n",
    "# path where we will save the trained model\n",
    "model_trig_path = statedicts_dir / \"task_1-2_model_trig.pt\"\n",
    "\n",
    "\n",
    "def evaluate_model_test_data_trig(_model, _test_loader):\n",
    "    running_loss_test = 0.0\n",
    "    running_error_test = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data_test in enumerate(_test_loader):\n",
    "            count += 1\n",
    "            inputs_test, labels_test = data_test\n",
    "            batch = labels_test.shape[0]\n",
    "            est = _model(inputs_test)\n",
    "            est_th = trig_to_theta(est)\n",
    "            label_th = trig_to_theta(labels_test)\n",
    "            loss_test = loss_fn(est_th, label_th)\n",
    "            error_test = error_fn(est_th, label_th)\n",
    "            running_loss_test += loss_test.item()\n",
    "            running_error_test += error_test.item()\n",
    "    print(\n",
    "        f\"Loss on test data: {running_loss_test / count:.3f}, Prediction error of model on test data: {running_error_test / count:.3f}\"\n",
    "    )\n",
    "\n",
    "    return running_error_test / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0dc0c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "260b553bb242246ce25189399d31c569",
     "grade": true,
     "grade_id": "cell-68238795be7ee278",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1.2: TRAIN MODEL HERE\"\"\"\n",
    "    num_runs = 10\n",
    "    pred_error_trig = onp.zeros((num_runs))\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        model_trig = NeuralNetworkTrig()\n",
    "        optimizer = torch.optim.SGD(model_trig.parameters(), lr=1e-3)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        print(f\"run {run+1} finished\")\n",
    "        pred_error_trig[run] = evaluate_model_test_data_trig(\n",
    "            model_trig, test_loader_trig\n",
    "        )\n",
    "    print(\"average prediction error: \", onp.mean(pred_error_trig))\n",
    "    \"\"\"TASK 1.2: END\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b98054",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "398198cb814377fb736f20933586a5d3",
     "grade": false,
     "grade_id": "cell-927636395afa22bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If working on CodeSpaces, run this cell before running the next model to free up memory\n",
    "if not AUTOGRADING:\n",
    "    # save the jitted model of the last run\n",
    "    model_trig_scripted = torch.jit.script(model_trig)\n",
    "    model_trig_scripted.save(str(model_trig_path))\n",
    "\n",
    "    %xdel train_loader_trig\n",
    "    %xdel val_loader_trig\n",
    "    %xdel test_loader_trig\n",
    "    %xdel model_trig\n",
    "    %xdel model_trig_scripted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a06631",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dc6c61ebe66390ae94b58a0f58074e8",
     "grade": false,
     "grade_id": "cell-537ac57da9947bb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    # load jitted model of the last run\n",
    "    model_trig_scripted = torch.jit.load(str(model_trig_path))\n",
    "\n",
    "    _, test_dataset_trig = load_test_dataset_dataloader(\n",
    "        datasets_dir / \"dataset_double_pendulum_test.npz\",\n",
    "        _rng=init_rng,\n",
    "        model_type=\"trig\",\n",
    "    )\n",
    "\n",
    "    evaluate_model(\n",
    "        model_trig_scripted,\n",
    "        test_dataset_trig,\n",
    "        model=\"trig\",\n",
    "        file=\"task_1-2_model_trig_prediction_error.pdf\",\n",
    "    )\n",
    "\n",
    "    %xdel test_dataset_trig\n",
    "    %xdel model_trig_scripted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f63b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55f88a15dfd4da8210da2406d6deef76",
     "grade": false,
     "grade_id": "cell-f481bd08fe4f5191",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 - Analyse Model performance (4p)\n",
    "Compare the prediction estimates for $M_{trig}$ with the plot for $M_θ$. Why does indirectly predicting the angle improve the prediction accuracy? **(2p)** (Answer in the cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c44b2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76513491faa4e3f5053dcd14e473ffb3",
     "grade": true,
     "grade_id": "cell-2815ae8a80e8eb78",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377b1a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e152dc29dcb3582ed894bbcbe7703f07",
     "grade": false,
     "grade_id": "cell-dc170e69aa93f07f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Why is it not sufficient to predict only sin(θ) and use its inverse θ = arcsin(sin(θ)) to get an estimate of the angle? **(2p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8b427",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a767cac6dceb52e46692aed13595d95",
     "grade": true,
     "grade_id": "cell-dae1ce5f27eb6216",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a1cfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8dbfb868b40c0f2a45d220ec9ff0176d",
     "grade": false,
     "grade_id": "cell-eb4b321fb6188424",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.3: Indirectly predict the angles with a Convolutional Neural Network (10)\n",
    "Instead of using a vanilla fully-connected neural network, we will build a prediction model $M_{cnn}$ that uses a convolutional neural network (CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f81751",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd426cbb8c61317823abac79e7e1e66a",
     "grade": false,
     "grade_id": "cell-505fc74a22f4805f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 - Prepare the data\n",
    "\n",
    "Run the functions below to load the data generated in `task_1a_generate_data.ipynb`, split the training into the 70/30 train/validation split, set the batch size to and put them in a dataloader. The data is first put in the class `CNNDataset`. This class again converts the `uint8` pixel values from a range of 0-255 to float values in the range `0-1` while also calculating the values of $\\mathrm{sin}(\\theta_1), \\mathrm{sin}(\\theta_2), \\mathrm{cos}(\\theta_1), \\mathrm{cos}(\\theta_2)$ to be used as labels instead of just $\\theta_1, \\theta_2$ as was done in the `ThetaDataset` class. We also permutate the order of the pixel ovbservations from `(number of samples, pixel rows, pixel columns, color channels)` to `(number of samples, color channels, pixel rows, pixel columns, )` as this is the order that the 2d convolution layer expects. The dataloader is again set to shuffle the data randomly before enumerating during the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db63cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b44964c85bb36d199843f1e42670568",
     "grade": false,
     "grade_id": "cell-ba0ddd02786cf10e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    train_loader_cnn, val_loader_cnn = load_training_dataset_dataloader(\n",
    "        datasets_dir / \"dataset_double_pendulum_train.npz\",\n",
    "        _rng=init_rng,\n",
    "        model_type=\"cnn\",\n",
    "    )\n",
    "    test_loader_cnn, _ = load_test_dataset_dataloader(\n",
    "        datasets_dir / \"dataset_double_pendulum_test.npz\",\n",
    "        _rng=init_rng,\n",
    "        model_type=\"cnn\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2dd6c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dd582b3dc962a5cfc28582618bf9fa7",
     "grade": false,
     "grade_id": "cell-44f9f0eecd679b24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 - Create the model (2p)\n",
    "Again, create sequential PyTorch model class and define the following architecture with Convolutional and pooling layers. \n",
    "- Start with a CNN layer `torch.nn.Conv2d(...)` with 32 filters, kernel size of `3x3` and a ReLU activation function.\n",
    "- Then add a max pooling layer `torch.nn.MaxPool2d(...)` with a pool size of `2x2`.\n",
    "- Add another convolutional layer with 10 filters and a ReLU activation function.\n",
    "- Then add another max pooling layer `torch.nn.MaxPool2d(...)` with a pool size of `2x2`.\n",
    "- Flatten the output of the pooling layer\n",
    "- Add a fully connected layer of 30 with a relu activation function.\n",
    "- Finally, add a fully connected layer without activation. Remember, the number of ouput units must match the dimension of the target data, which is 4 as we will predict the trigonometric function again. \n",
    "\n",
    "**note:* No zero padding is needed as the images have enough whitespace at the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b4f02",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9521a1b51f4f9d3769c716c3e6b515e",
     "grade": true,
     "grade_id": "cell-b5d85df96369ff97",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TASK 1.3: CREATE MODEL HERE\"\"\"\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\"\"\"TASK 1.3: END HERE\"\"\"\n",
    "\n",
    "model_cnn = NeuralNetworkCNN()\n",
    "total_params_cnn = sum(p.numel() for p in model_cnn.parameters())\n",
    "print(\"total number of model parameters: \", total_params_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0d850",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "086f3d6a3395f08b5496fb734a64f6dd",
     "grade": false,
     "grade_id": "cell-ad3641dc5e865e94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 - Train the model (2p)\n",
    "\n",
    "Train the model with the same training parameters defined below as in task 1.1 and 1.2 but using the training data `train_loader_cnn`. Again, Tune the learning rate in the optimizer to give the best performance on the validation and test set over the 100 epochs. Do this 10 times and record the prediction error of the final model of each of the 10 runs on the test dataset `test_loader_cnn` using the function `evaluate_model_test_data_cnn`. Don't forget to reinitialize the parameters on each run! **Hints:** \n",
    "- reduce the number of epochs and lower the number of runs to `1` while getting your model working. \n",
    "- The try in the range of 1e-1 to 1e-5 for the learning rate\n",
    "\n",
    "#### One last thing!\n",
    "A Convolutional neural network that can predict angles is required for task 2b. Therefore, focus getting the perfect learning and optimal model accuracy with this Neural Network to see how far you can increase the prediction accuracy of link angles. Be warned, the accuracy of your model will affect the performance of you controller in task 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "optimizer = torch.optim.SGD(model_cnn.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "error_fn = torch.nn.L1Loss()\n",
    "num_epochs = 100\n",
    "\n",
    "# path where we will save the trained model\n",
    "model_cnn_path = statedicts_dir / \"task_1-3_model_cnn.pt\"\n",
    "\n",
    "\n",
    "def evaluate_model_test_data_cnn(_model, _test_loader):\n",
    "    \"\"\"\n",
    "    evaluates the model_cnn on the\n",
    "    \"\"\"\n",
    "    running_loss_test = 0.0\n",
    "    running_error_test = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data_test in enumerate(_test_loader):\n",
    "            count += 1\n",
    "            inputs_test, labels_test = data_test\n",
    "            batch = labels_test.shape[0]\n",
    "            est = _model(inputs_test)\n",
    "            est_th = trig_to_theta(est)\n",
    "            label_th = trig_to_theta(labels_test)\n",
    "            loss_test = loss_fn(est_th, label_th)\n",
    "            error_test = error_fn(est_th, label_th)\n",
    "            running_loss_test += loss_test.item()\n",
    "            running_error_test += error_test.item()\n",
    "\n",
    "    print(\n",
    "        f\"Loss on test data: {running_loss_test / count:.3f}, Prediction error of model on test data: {running_error_test / count:.3f}\"\n",
    "    )\n",
    "    return running_error_test / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca36dc7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18e9d687775e5e564ad22b86fd1a6f16",
     "grade": true,
     "grade_id": "cell-cf838bf5bcf84bdb",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1.3: TRAIN MODEL HERE\"\"\"\n",
    "\n",
    "    num_runs = 1\n",
    "    pred_error_cnn = onp.zeros((num_runs))\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        model_cnn = NeuralNetworkCNN()\n",
    "        optimizer = torch.optim.SGD(model_cnn.parameters(), lr=1e-1)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        print(f\"run {run+1} finished\")\n",
    "        pred_error_cnn[run] = evaluate_model_test_data_cnn(model_cnn, test_loader_cnn)\n",
    "    print(\"average prediction error: \", onp.mean(pred_error_cnn))\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "    \"\"\"TASK 1.3: END\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eaed41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85600a1b6de30560caa9a89801d718fd",
     "grade": false,
     "grade_id": "cell-41fc55986ec705e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If working on CodeSpaces, run this cell before running the next model to free up memory\n",
    "if not AUTOGRADING:\n",
    "    # save the jitted model of the last run\n",
    "    model_cnn_scripted = torch.jit.script(model_cnn)\n",
    "    model_cnn_scripted.save(str(model_cnn_path))\n",
    "\n",
    "    %xdel train_loader_cnn\n",
    "    %xdel val_loader_cnn\n",
    "    %xdel test_loader_cnn\n",
    "    %xdel model_cnn\n",
    "    %xdel model_cnn_scripted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5de0e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ebc259dbc2069796120d72120fd3d39",
     "grade": false,
     "grade_id": "cell-4f2f9c81324c39b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    # load jitted model of the last run\n",
    "    model_cnn_scripted = torch.jit.load(str(model_cnn_path))\n",
    "\n",
    "    _, test_dataset_cnn = load_test_dataset_dataloader(\n",
    "        datasets_dir / \"dataset_double_pendulum_test.npz\",\n",
    "        _rng=init_rng,\n",
    "        model_type=\"cnn\",\n",
    "    )\n",
    "\n",
    "    evaluate_model(\n",
    "        model_cnn_scripted,\n",
    "        test_dataset_cnn,\n",
    "        model=\"cnn\",\n",
    "        file=\"task_1-3_model_cnn_prediction_error.pdf\",\n",
    "    )\n",
    "\n",
    "    %xdel test_dataset_cnn\n",
    "    %xdel model_cnn_scripted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e5145",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c25c09788fbb8230aa75a0ceaace2a6",
     "grade": false,
     "grade_id": "cell-a5bc36d52b583442",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 - Analyse and Compare Model performance \n",
    "Make a comparison of the different models (i.e. M θ, M trig, M cnn) based on the average prediction accuracy\n",
    "on the test dataset and the number of trainable parameters. Which model would you prefer and why? (answer in the cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88887c50",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63b207d5e7a1ae467cf7f674fc199c2e",
     "grade": true,
     "grade_id": "cell-e2d3ff8fab5701aa",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02731fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c145f5b06628fb57436f5c063db3ddc",
     "grade": false,
     "grade_id": "cell-13a942a4eeaa6fcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you change the activation of the last fully connected layer to ReLU, the prediction accuracy completely deteriorates. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864b2bf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f9c152abcbff44ee619c7c6a1bde535",
     "grade": true,
     "grade_id": "cell-d18c9ec4e98b87c4",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80035d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1b972003f280e0432a7c8abcc34fc86",
     "grade": false,
     "grade_id": "cell-6d6768e940504fa9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.4: Variance across runs (2.5p) \n",
    "In the previous tasks, the prediction accuracy varied across different runs even though the underlying code and dataset remained unchanged.\n",
    "\n",
    "Use the code cell below to set the seed for PyTorch. Train the model for a seed of 0 and then for seed of 1, recording the average prediction accuracy and standard deviation for both.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf44a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea9d7dc272b251ab005c3349f7db0b62",
     "grade": false,
     "grade_id": "cell-7614554b44ddc131",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Important: only run this cell once as it resets your results for all seeds\n",
    "# initialize dictionary to keep track of prediction error across seeds\n",
    "pred_error_sds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3206c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to either 0 or 1\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca78ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5786b377792f4673531702e631881ab3",
     "grade": false,
     "grade_id": "cell-1910ebea89f95d4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Copy the architecture of $M_{\\theta}$ to create a new model below. Don't forget to comment on the code to set the seed again when working on the other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c25ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a656a453e8d3ce79bd55eec8f0d8a90",
     "grade": true,
     "grade_id": "cell-cc04b732f5935e1b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" TASK 1.4: CREATE MODEL HERE \"\"\"\n",
    "\n",
    "\n",
    "class NeuralNetworkThetaSeed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\"\"\"TASK1.4: END\"\"\"\n",
    "model_theta_seed = NeuralNetworkThetaSeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aef70d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdca4b6758786aeb9d924efe73d4e4fc",
     "grade": false,
     "grade_id": "cell-ab56a3b7cc55c6c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Set up a training loop as done for the task 1.3 and run the training 10 times while having the seed value as 0 and then run it again for a seed of 1. Use the same datasets as in task 1.3. **(0.5p)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73ad58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e70338deae655254c55cb65b43835ea4",
     "grade": false,
     "grade_id": "cell-367271a0f9c5ec5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    train_loader_theta, val_loader_theta = load_training_dataset_dataloader(\n",
    "        str(datasets_dir / \"dataset_double_pendulum_train.npz\"),\n",
    "        _rng=init_rng,\n",
    "        model_type=\"theta\",\n",
    "    )\n",
    "    test_loader_theta, _ = load_test_dataset_dataloader(\n",
    "        str(datasets_dir / \"dataset_double_pendulum_test.npz\"),\n",
    "        _rng=init_rng,\n",
    "        model_type=\"theta\",\n",
    "    )\n",
    "\n",
    "# Training parameters\n",
    "optimizer = torch.optim.SGD(model_theta_seed.parameters(), lr=1e-3)\n",
    "# We choose the mean square loss as it is a regression problem\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "num_epochs = 30\n",
    "\n",
    "# define the path to save the model to\n",
    "model_theta_seed_path = str(statedicts_dir / f\"task_1-4_model_theta_seed-{seed}.pt\")\n",
    "\n",
    "num_runs = 10\n",
    "pred_error_sds[seed] = onp.zeros((num_runs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca4c83",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13d8007cb604d7d701adf40f19c5e968",
     "grade": true,
     "grade_id": "cell-184b3cffa2fc5614",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1.4: TRAIN MODEL HERE\"\"\"\n",
    "    for run in range(num_runs):\n",
    "        torch.manual_seed(seed=seed)\n",
    "        model_theta_seed = NeuralNetworkThetaSeed()\n",
    "        optimizer = torch.optim.SGD(model_theta_seed.parameters(), lr=1e-3)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \"\"\"TASK 1.4: END\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba49d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e84b0e8442b211e4af3d37e26e34dae",
     "grade": false,
     "grade_id": "cell-57570ffac5d0711e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If working on CodeSpaces, run this cell before running the next model to free up memory\n",
    "if not AUTOGRADING:\n",
    "    # jit the model of the last run\n",
    "    model_theta_seed_scripted = torch.jit.script(model_theta_seed)\n",
    "\n",
    "    # save the jitted model of the last run\n",
    "    model_theta_seed_scripted.save(str(model_theta_seed_path))\n",
    "\n",
    "    %xdel train_loader_theta\n",
    "    %xdel val_loader_theta\n",
    "    %xdel test_loader_theta\n",
    "    %xdel model_theta_seed\n",
    "    %xdel model_theta_seed_scripted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189eda9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a95617240f141b337934b8c01f5e8fee",
     "grade": false,
     "grade_id": "cell-9d6eb36d0e5ce560",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    # load the jitted model of the last run of the currently selected seed\n",
    "    model_theta_seed_scripted = torch.jit.load(str(model_theta_seed_path))\n",
    "\n",
    "    _, test_dataset_theta = load_test_dataset_dataloader(\n",
    "        str(datasets_dir / \"dataset_double_pendulum_test.npz\"),\n",
    "        _rng=init_rng,\n",
    "        model_type=\"theta\",\n",
    "    )\n",
    "\n",
    "    evaluate_model(\n",
    "        model_theta_seed_scripted,\n",
    "        test_dataset_theta,\n",
    "        model=\"theta\",\n",
    "        file=f\"task_1-4_model_theta_prediction_error_seed-{seed}.pdf\",\n",
    "    )\n",
    "\n",
    "    %xdel test_dataset_theta\n",
    "    %xdel model_theta_seed_scripted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553da8ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9ff2a2ed28bdb7bb19b1b5bde1f7d5a",
     "grade": false,
     "grade_id": "cell-27be995a7917e630",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- What do you observe? (1p)\n",
    "\n",
    "- What is the benefit of seeding the pseudo-random generator in practice? **(1p)**\n",
    "\n",
    "Write your answers in the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3bc99",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95a86a8351d6962fd6620a6712451725",
     "grade": true,
     "grade_id": "cell-47bf48a2d9c4af1d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f6bbc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08c900766c933fd0e86705af053ce509",
     "grade": false,
     "grade_id": "cell-cec950b501583524",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1.5: Spiking neural networks (15p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2de81e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08ae3a1528be8f96f48c104867175721",
     "grade": false,
     "grade_id": "cell-3d456e0fbb95d0e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If our goal is to control the double pendulum, the angular velocity $\\dot{\\theta}$ is generally also required. However, temporal information cannot be extracted from individual images. We are going to utilize the temporal advantage of the spiking neural networks (SNN) to predict the angular velocity of each link. Instead of static individual images, we adopt event-based data as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8128f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4d5b26ce4148c3dd6acdaa59ed30062",
     "grade": false,
     "grade_id": "cell-013353c1c5418c4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 - Warm Up\n",
    "Use the Jupyter notebook `task_1-5_SNN_warmup.ipynb` to understand the neuron structure of SNNs, construct a LIF neuron model and a simple fully connected SNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b95a69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "689db5dfe7020b9e10952d935936961a",
     "grade": false,
     "grade_id": "cell-ca0b905a64e10e55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 - Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83d06d",
   "metadata": {},
   "source": [
    "In this part, the dataloader has been defined, and the two data sets `train_loader_snn` and `test_loader_snn` have been generated, which can be directly used to train the network. The two datasets are the same size. The batch_size has chosen 16. And we shuffle the dataset by setting shuffle to true. At the same time, to make the size of the data set just divisible by batch_size, we set drop_last to true to discard the last set of data that is less than one batch. At the same time, we did not set the validation set.\n",
    "\n",
    "Therefore, the size of each piece of data in dataloader is: （batch_size: 16, time_step: 20, channels: 2, size: 32, size: 32）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a879a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af1c68b5e06dd1d0eaa9687a29efc95c",
     "grade": false,
     "grade_id": "cell-98b1690578b3fdf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This part has to be same as task_1a_generate_data.ipynb\n",
    "train_th1_range = jnp.arange(-jnp.pi / 6.0, jnp.pi / 6.0, jnp.pi / 30.0)\n",
    "train_th2_range = jnp.arange(-jnp.pi, jnp.pi, jnp.pi / 6.0)\n",
    "test_th1_range = jnp.arange(-jnp.pi / 6.0, jnp.pi / 6.0, jnp.pi / 30.0)\n",
    "test_th2_range = jnp.arange(-jnp.pi, jnp.pi, jnp.pi / 6.0)\n",
    "\n",
    "TRAIN_NUM_DATA = len(train_th1_range) * len(train_th2_range)\n",
    "TEST_NUM_DATA = len(test_th1_range) * len(test_th2_range)\n",
    "NUM_SNN_DATA = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2008b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6328b8fae49aa7697c3cb31f20eb5aa0",
     "grade": false,
     "grade_id": "cell-7dbf1ba461ec8298",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    train_set_snn = SNNDataset(\n",
    "        datasets_dir / \"event_based_data\" / \"train\",\n",
    "        TRAIN_NUM_DATA,\n",
    "        NUM_SNN_DATA,\n",
    "    )\n",
    "    train_loader_snn = DataLoader(\n",
    "        train_set_snn, batch_size=16, shuffle=True, drop_last=True\n",
    "    )\n",
    "\n",
    "    test_set_snn = SNNDataset(\n",
    "        datasets_dir / \"event_based_data\" / \"test\",\n",
    "        TEST_NUM_DATA,\n",
    "        NUM_SNN_DATA,\n",
    "    )\n",
    "    test_loader_snn = DataLoader(\n",
    "        test_set_snn, batch_size=16, shuffle=True, drop_last=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630360d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0648b8183773e05fb9c2d15bfb3be94e",
     "grade": false,
     "grade_id": "cell-62b53199c8d6ae6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 - Create the model (4p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b79751",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b11c8266dd4d03278bb58e36fc8b0e9",
     "grade": false,
     "grade_id": "cell-25520c2578c97471",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We create a class with the name `SnnModel` extending `nn.Module`. Please complete the implementation below. In thi case, we adopt the LIF model for neurons.\n",
    "\n",
    "The structure of the network is as follows:\n",
    "\n",
    "* Start with a convolutional layer with $16$ output channels and a kernel size of `5x5`.\n",
    "* Then, add a max pooling layer with a pooling size of `4x4`.\n",
    "* Add a lif operation to convert the input current from the convolution into output spikes.\n",
    "* Add the second convolutional layer with $16$ as output channel size, kernel size of `3x3`, then add a lif operation.\n",
    "* Add the third convolutional layer with $16$ as output channel size, kernel size of `3x3`, then add a lif operation.\n",
    "* All above parts are in a recurrent way. Then, the SNN part is finished. All last layer's membrane potential in each time step will be stacked together to get the SNN result.\n",
    "* This final step is to process the SNN's results as usual. Flatten the obtained membrane potential and feed it into two fully connected layers with the ReLU activation fuction in between. We recommend the input size of the 2nd fully connected layer to be $640$. The final output number must match the dimension of our target data which is 2.\n",
    "\n",
    "Meanwhile for LIF model, we use a `fast_sigmoid` function as the surrogate function, and set the decay rate `beta` to $0.8$. It is worth noting that we need to use a recurrent approach to process event-based data. Therefore,\n",
    "\n",
    "* we should use the for-loop to traverse each time step of the input event-based data. For the data of each time step, we can use the convolution operation to process the data as usual. Each convolution operation is followed by a layer of SNN neurons. Finally, the membrane potential of the last layer of all time steps is used as the output. Therefore, the expected output format of the SNN part should be `[batch_size: 16, time_step: 20, conv_channel: (should be the channel size of the last convolutional layer), size: (output size), size: (output size)]`.\n",
    "* The first dimension of the input data is batch size, but not the time steps. So we need to transpose the input tensor first and then use for-loop to process it.\n",
    "* The final non-linear layer (two fully connected layers with ReLU) should be the conventional neurons instead of SNN neurons.\n",
    "* Do not forget to initialize the LIF neurons!\n",
    "\n",
    "**Note**: What's the differences between the fully connected (FC) layer here compared to the fully connected layer in the SNN structure in `task_1-5_SNN_warmup.ipyn`?\n",
    "\n",
    "The FC layer in fully-connected SNN structure is used to build a SNN model to process the input spikes. So it should be implemented in the for-loop, and followed by a lif operation (LIF neuron models). However, the FC layer in this assignment is used to map the last  membrane potential steps to the final output. So it should be implemented outside of the for-loop and in the conventional way (i.e. like in the conventional neuron models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb92e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33f52419eba4333095be87fb76544ffa",
     "grade": true,
     "grade_id": "cell-d96a62b92b1c8df6",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\"\"\"TASK1.5: END\"\"\"\n",
    "\n",
    "model_snn = SnnModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbc824",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffd0ee3e51ad67893f73f66391591ebb",
     "grade": false,
     "grade_id": "cell-7238196d33aa84de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 - Train the model (3p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d7b20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d58888e29146a3cea2e25d539f95d6a",
     "grade": false,
     "grade_id": "cell-64f390c4e4d9ef2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Train the SNN model in the same way with other models. We recommend using `1e-3` as the learning rate and training the model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c630159",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65f33d5f5980bdd44ea8220ca6ec08e5",
     "grade": false,
     "grade_id": "cell-733f37b346b1f159",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_snn.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77dbc0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08255c619d4b2d3e9abfa5585c0c2947",
     "grade": true,
     "grade_id": "cell-e365b12765bd52a2",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "onp.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1.5: TRAIN MODEL HERE\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \"\"\"TASK 1.5: END\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56685dd3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e55a9e0a13ac07c59185f30a9ae45ef6",
     "grade": false,
     "grade_id": "cell-ff2497b66bbcf9f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the SNN model with test data\n",
    "if not AUTOGRADING:\n",
    "    torch.save(model_snn.state_dict(), str(statedicts_dir / \"task_1-5_model_snn.pt\"))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_snn = model_snn.eval()\n",
    "        print_loss = 0.0\n",
    "        count = 0\n",
    "        for i, (events, targets) in enumerate(test_loader_snn):\n",
    "            output = model_snn(events)\n",
    "            loss = loss_fn(output, targets)\n",
    "            print_loss += loss.item()\n",
    "            count = count + 1\n",
    "\n",
    "        print_loss = print_loss / count\n",
    "        print(f\"The MSE loss of the model on the test dataset is: {print_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8904d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ee4b4860a9462b1cb64b2b659d6c119",
     "grade": false,
     "grade_id": "cell-501c8e5f33a32e3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 - Analysis of results (8p)\n",
    "\n",
    "Does your loss decrease step by step? During the training process, is the training speed faster or slower comparing to CNN and why? Please analyse the advantages and disadvantages of SNN. **(8p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abd8cc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cffd86a98bfc91a4cfec7158d8ccd88b",
     "grade": true,
     "grade_id": "cell-b9f9afb31f989574",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f9ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
